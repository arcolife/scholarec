#!/usr/bin/env python
# -*- coding: utf-8 -*- 

## This file is part of ScholaRec.
## A recommendation engine for Scholarly works.
## Copyright (C) 2014  Archit Sharma <archit.py@gmail.com>
##
## ScholaRec is free software: you can redistribute it and/or modify
## it under the terms of the GNU General Public License as published by
## the Free Software Foundation, either version 3 of the License, or
## (at your option) any later version.
##
## ScholaRec is distributed in the hope that it will be useful,
## but WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
## GNU General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with this program.  If not, see <http://www.gnu.org/licenses/>

""" This module tests the scholarec modules by
implementing requests to query arXiv API."""

## import general modules
import os
import sys
from urllib2 import \
    urlopen, quote
import json
from pprint import \
    pprint, pformat
import wikipedia

## Import dependencies specific to scholarec
from scholarec import DocumentArXiv
SOURCE_PATH = os.path.dirname(os.path.abspath(__file__)) + '/../'
sys.path.insert(0, SOURCE_PATH)

'''
# Open saved XML file
file = open(SOURCE_PATH + 'log/output/query.xml','r').read()
file.close() 
'''

class QueryParse:
    ''' mathods to parse query '''    
    def __init__(self, query_xml, keywords):
        self.query_xml = query_xml
        self.entry_count = None
        self.data_dict = None
        self.keywords = keywords
        self.Doc = None
        
    def parse_data(self):
        """ parse extracted data into python dict format """
        self.Doc = DocumentArXiv(self.query_xml)
        #self.data_xml, self.data_dict = Doc.extract_data()
        self.data_dict = self.Doc.extract_tags()
        self.entry_count = len(self.data_dict.keys())
        if self.entry_count:
            print "Total Entries: ", self.entry_count
        else:
            sys.exit("\nNo matching entries found!")
        
    def print_data(self):
        """ To print entries """
        for entry_id in self.data_dict.keys():
            print "ID: %s"%(entry_id)
            pprint(self.data_dict[entry_id])
            print
        '''
        # if data type were XML
        for i in xrange(entry_count):
            print "Entry %d: "%(i+1),"\n"
            print self.data_xml[i].toxml()
            print "\n-----------------\n"
        '''

    def store_data(self):
        """ write response to enternal file """
        # write to JSON
        self.data_json = json.JSONEncoder().encode(self.data_dict)
        f_json = open( SOURCE_PATH + 'log/output/query_results.json','wb')
        f_json.write(self.data_json)
        f_json.close()
        # write to XML
        f_xml = open( SOURCE_PATH + 'log/output/query_results.xml','wb')
        f_xml.write(self.query_xml)
        f_xml.close()

    def generate_html(self):
        # generates an html to read results in browser in raw form
        html_code = """<html>
        <head>
        <title>ScholaRec produced the following search response</title>
        <h1>Results for keywords: %s</h1>
        </head>
        <body> """ % (self.keywords)
        html_code += '<pre>' + pformat(self.data_dict) + '</pre>'
        html_code += """</body></html>"""        
        # write HTML
        f = open( SOURCE_PATH + 'public/results_simple.html', 'wb')
        f.write(html_code)
        f.close()
        # write results as a text file
        f = open( SOURCE_PATH + 'public/results.txt', 'wb')
        f.write( str(self.data_dict) )
        f.close()

    def store_pdfs(self):
        # retrieve and extract pdfs
        print "Now downloading all pdf's .. "
        self.Doc.extract_pdfs()

if __name__ == '__main__':
    try:

        keywords = raw_input("\nEnter keywords, to query arXiv: ")
        start = str(int(raw_input("Enter pagination start (0,1,..): ")))
        search = "http://export.arxiv.org/api/query?search_query=all:" \
                 + '+'.join(keywords.split()) + "&start=" + start + "&max_results=" \
                + str(int(raw_input("Enter maximum result count: ")))

        print "\n\t\tPlease wait for query response.."
        search_response = urlopen(search)
        assert search_response.msg.upper() == 'OK'        
        print "\t\tResponse: OK\n"
        query_xml = search_response.read()
        Qp = QueryParse(query_xml, keywords)
        Qp.parse_data()

        if raw_input("\nPrint entries? (y/n): ").lower() == 'y':
            # print data
            Qp.print_data()
        if raw_input("\nStore query response to ./log/output? (y/n): ").lower() == 'y':
            # store data        
            Qp.store_data()
        if raw_input("\nExtract PDF's & plain texts? (y/n): ").lower() == 'y':
            # retrieve pdfs & store them
            Qp.store_pdfs()
        if raw_input("\nGenerate HTML in public/results.html? (y/n): ").lower() == 'y':
            # retrieve pdfs & store them
            Qp.generate_html()

        print "\nYou've searched for : %s \n" %(keywords)
        print "\nWikipedia says: \n %s \n" % (wikipedia.summary(keywords))
        print "\nEnter following URL in browser, for GUI mode: \n %s \n" % (search) 
        print "\n\t\tThanks for using ScholaRec!\n"

    except IOError as e:
        print "\nI/O error({0}): {1}".format(e.errno, e.strerror)
        print "Maybe you're disconnect from the Internet.."
    except KeyboardInterrupt:
        print "\n\n\t\tYou've aborted the program! \n"
    except ValueError as e:
        print "\nERROR: ",e,"\nCheck your input type."
    except IndexError as e:
        print "\nERROR: ",e,"\nResponse attributes not satisfied in data_dict."
    except AssertionError as e:
        print "\nERROR: Response: not OK! Maybe a site error"
    except:
        # >>sys.exc_info()<< gives whole exception
        print "Error: ", sys.exc_info()[0]  
        # system call to raise the exception out loud
        raise
